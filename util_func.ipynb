{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a6ae880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709f2b68",
   "metadata": {},
   "source": [
    "# 1 logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbe8dd81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from  tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3ed8ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(filename, verbosity=1, name=None):\n",
    "    level_dict = {0: logging.DEBUG, 1: logging.INFO, 2: logging.WARNING}\n",
    "    formatter = logging.Formatter(\n",
    "        \"[%(asctime)s][%(filename)s][line:%(lineno)d][%(levelname)s] %(message)s\"\n",
    "    )\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level_dict[verbosity])\n",
    "\n",
    "    fh = logging.FileHandler(filename, \"w\")\n",
    "    fh.setFormatter(formatter)\n",
    "    logger.addHandler(fh)\n",
    "\n",
    "    sh = logging.StreamHandler()\n",
    "    sh.setFormatter(formatter)\n",
    "    logger.addHandler(sh)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead1e695",
   "metadata": {},
   "source": [
    "# 2 evaluate utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2604e4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y):\n",
    "    y_pred = torch.argmax(y_pred,-1)\n",
    "    return sum(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0d072c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Accumulator:\n",
    "    def __init__(self,n):\n",
    "        self.data = [0.0]*n\n",
    "    \n",
    "    def add(self,*args):\n",
    "        self.data = [a+float(b) for a,b in zip(self.data,args)]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.data = [0.0]*n\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5de8b1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_acc(net,data_iter):\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.eval()\n",
    "    metric = Accumulator(2)\n",
    "    with torch.no_grad():\n",
    "        for x,y in data_iter:\n",
    "            pred_y = net(y)\n",
    "            metric.add(accuracy(y_pred,y),y.numel())\n",
    "    return metric[0]/metric[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8679937a",
   "metadata": {},
   "source": [
    "# 3 train utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c0388ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ad1b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(data_iter,net,loss,optimize):\n",
    "    if isinstance(net,torch.nn.Module):\n",
    "        net.train()\n",
    "    metric = Accumulator(3)\n",
    "    for x,y in data_iter:\n",
    "        y_pred = net(x)\n",
    "        loss_tmp = loss(y_pred,y)\n",
    "        metric.add(loss_tmp*len(y),accuracy(y_pred,y),y.numel())\n",
    "        ##optimize\n",
    "        optimize.zero_grad()\n",
    "        loss_tmp.backward()\n",
    "        optimize.step()\n",
    "    return metric[0]/metric[2],metric[1]/metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "163a84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_iter,test_iter,net,loss,optimize,num_epochs,logger):\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_metrics = train_epoch(train_iter,net,loss,optimize)\n",
    "        test_acc = evaluate_acc(net,test_loader)\n",
    "        print('epoch:%d\\ttrain_loss:%f\\ttrain_acc:%f\\ttest_acc:%f'%(epoch,train_metrics[0],train_metrics[1],test_acc))\n",
    "        logger.info('epoch:%d\\ttrain_loss:%f\\ttrain_acc:%f\\ttest_acc:%f'%(epoch,train_metrics[0],train_metrics[1],test_acc))\n",
    "    train_loss,train_acc = train_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e504c3",
   "metadata": {},
   "source": [
    "# 4 optimize set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7432878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdamW (\n",
      "Parameter Group 0\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-08\n",
      "    initial_lr: 3e-05\n",
      "    lr: 3e-05\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    betas: (0.9, 0.999)\n",
      "    correct_bias: True\n",
      "    eps: 1e-08\n",
      "    initial_lr: 3e-05\n",
      "    lr: 3e-05\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "Debug optimi {'state': {}, 'param_groups': [{'weight_decay': 0.0, 'lr': 3e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'correct_bias': True, 'initial_lr': 3e-05, 'params': [0]}, {'weight_decay': 0.0, 'lr': 3e-05, 'betas': (0.9, 0.999), 'eps': 1e-08, 'correct_bias': True, 'initial_lr': 3e-05, 'params': [1]}]}\n",
      "Debug!!!sche {'base_lrs': [3e-05, 3e-05], 'last_epoch': 0, '_step_count': 1, 'verbose': False, '_get_lr_called_within_step': False, '_last_lr': [3e-05, 3e-05], 'lr_lambdas': [None, None]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = torch.nn.Sequential(torch.nn.Linear(in_features=100,out_features=2))\n",
    "\n",
    "##optimize\n",
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "{\"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "\"weight_decay\": 0.0,\n",
    "},\n",
    "{\n",
    "    \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "    \"weight_decay\": 0.0,\n",
    "},\n",
    "]\n",
    "optimizer = transformers.AdamW( optimizer_grouped_parameters, \n",
    "                               lr=3e-5,betas=(0.9, 0.999),eps=1e-08)\n",
    "\n",
    "##learnign_rate adjust\n",
    "num_training_step=100\n",
    "lr_scheduler = transformers.optimization.get_scheduler(name='linear',\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_step\n",
    ")\n",
    "\n",
    "##grad clip\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
    "print(optimizer)\n",
    "print('Debug optimi',optimizer.state_dict())\n",
    "print('Debug!!!sche',lr_scheduler.state_dict())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876dab61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
